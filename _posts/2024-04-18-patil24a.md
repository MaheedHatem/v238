---
title: Failures and Successes of Cross-Validation for Early-Stopped Gradient Descent
abstract: We analyze the statistical properties of generalized cross-validation (GCV)
  and leave-one-out cross-validation (LOOCV) applied to early-stopped gradient descent
  (GD) in high-dimensional least squares regression. We prove that GCV is generically
  inconsistent as an estimator of the prediction risk of early-stopped GD, even for
  a well-specified linear model with isotropic features. In contrast, we show that
  LOOCV converges uniformly along the GD trajectory to the prediction risk. Our theory
  requires only mild assumptions on the data distribution and does not require the
  underlying regression function to be linear. Furthermore, by leveraging the individual
  LOOCV errors, we construct consistent estimators for the entire prediction error
  distribution along the GD trajectory and consistent estimators for a wide class
  of error functionals. This in particular enables the construction of pathwise prediction
  intervals based on GD iterates that have asymptotically correct nominal coverage
  conditional on the training data.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: patil24a
month: 0
tex_title: Failures and Successes of Cross-Validation for Early-Stopped Gradient Descent
firstpage: 2260
lastpage: 2268
page: 2260-2268
order: 2260
cycles: false
bibtex_author: Patil, Pratik and Wu, Yuchen and Tibshirani, Ryan
author:
- given: Pratik
  family: Patil
- given: Yuchen
  family: Wu
- given: Ryan
  family: Tibshirani
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/patil24a/patil24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
