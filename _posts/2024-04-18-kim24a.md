---
title: 'Linear Convergence of Black-Box Variational Inference: Should We Stick the
  Landing?'
abstract: We prove that black-box variational inference (BBVI) with control variates,
  particularly the sticking-the-landing (STL) estimator, converges at a geometric
  (traditionally called “linear”) rate under perfect variational family specification.
  In particular, we prove a quadratic bound on the gradient variance of the STL estimator,
  one which encompasses misspecified variational families. Combined with previous
  works on the quadratic variance condition, this directly implies convergence of
  BBVI with the use of projected stochastic gradient descent. For the projection operator,
  we consider a domain with triangular scale matrices, which the projection onto is
  computable in $\theta(d)$ time, where $d$ is the dimensionality of the target posterior.
  We also improve existing analysis on the regular closed-form entropy gradient estimators,
  which enables comparison against the STL estimator, providing explicit non-asymptotic
  complexity guarantees for both.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kim24a
month: 0
tex_title: 'Linear Convergence of Black-Box Variational Inference: Should We Stick
  the Landing?'
firstpage: 235
lastpage: 243
page: 235-243
order: 235
cycles: false
bibtex_author: Kim, Kyurae and Ma, Yian and Gardner, Jacob
author:
- given: Kyurae
  family: Kim
- given: Yian
  family: Ma
- given: Jacob
  family: Gardner
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/kim24a/kim24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
