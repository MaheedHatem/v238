---
title: Simulating weighted automata over sequences and trees with transformers
software: https://github.com/michaelrizvi/wfa2tf
abstract: 'Transformers are ubiquitous models in the natural language processing (NLP)
  community and have shown impressive empirical successes in the past few years. However,
  little is understood about how they reason and the limits of their computational
  capabilities. These models do not process data sequentially, and yet outperform
  sequential neural models such as RNNs. Recent work has shown that these models can
  compactly simulate the sequential reasoning abilities of deterministic finite automata
  (DFAs). This leads to the following question: can transformers simulate the reasoning
  of more complex finite state machines? In this work, we show that transformers can
  simulate weighted finite automata (WFAs), a class of models which subsumes DFAs,
  as well as weighted tree automata (WTA), a generalization of weighted automata to
  tree structured inputs. We prove these claims formally and provide upper bounds
  on the size of the transformer models needed as a function of the number of states
  of the target automata. Empirically, we perform synthetic experiments showing that
  transformers are able to learn these compact solutions via standard gradient-based
  training.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rizvi-martel24a
month: 0
tex_title: Simulating weighted automata over sequences and trees with transformers
firstpage: 2368
lastpage: 2376
page: 2368-2376
order: 2368
cycles: false
bibtex_author: Rizvi-Martel, Michael and Lizaire, Maude and Lacroce, Clara and Rabusseau,
  Guillaume
author:
- given: Michael
  family: Rizvi-Martel
- given: Maude
  family: Lizaire
- given: Clara
  family: Lacroce
- given: Guillaume
  family: Rabusseau
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/rizvi-martel24a/rizvi-martel24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
