---
title: Optimising Distributions with Natural Gradient Surrogates
software: https://github.com/cambridge-mlg/sngd
abstract: Natural gradient methods have been used to optimise the parameters of probability
  distributions in a variety of settings, often resulting in fast-converging procedures.
  Unfortunately, for many distributions of interest, computing the natural gradient
  has a number of challenges. In this work we propose a novel technique for tackling
  such issues, which involves reframing the optimisation as one with respect to the
  parameters of a surrogate distribution, for which computing the natural gradient
  is easy. We give several examples of existing methods that can be interpreted as
  applying this technique, and propose a new method for applying it to a wide variety
  of problems. Our method expands the set of distributions that can be efficiently
  targeted with natural gradients. Furthermore, it is fast, easy to understand, simple
  to implement using standard autodiff software, and does not require lengthy model-specific
  derivations. We demonstrate our method on maximum likelihood estimation and variational
  inference tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: so24a
month: 0
tex_title: Optimising Distributions with Natural Gradient Surrogates
firstpage: 2224
lastpage: 2232
page: 2224-2232
order: 2224
cycles: false
bibtex_author: So, Jonathan and E. Turner, Richard
author:
- given: Jonathan
  family: So
- given: Richard
  family: E. Turner
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/so24a/so24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
