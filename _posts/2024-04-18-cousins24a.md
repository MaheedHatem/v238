---
title: 'To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair Training
  on Shared Models'
abstract: In fair machine learning, one source of performance disparities between
  groups is overfitting to groups with relatively few training samples. We derive
  group-specific bounds on the generalization error of welfare-centric fair machine
  learning that benefit from the larger sample size of the majority group. We do this
  by considering group-specific Rademacher averages over a restricted hypothesis class,
  which contains the family of models likely to perform well with respect to a fair
  learning objective (e.g., a power-mean). Our simulations demonstrate these bounds
  improve over a na√Øve method, as expected by theory, with particularly significant
  improvement for smaller group sizes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cousins24a
month: 0
tex_title: 'To Pool or Not To Pool: Analyzing the Regularizing Effects of Group-Fair
  Training on Shared Models'
firstpage: 4573
lastpage: 4581
page: 4573-4581
order: 4573
cycles: false
bibtex_author: Cousins, Cyrus and Elizabeth Kumar, I. and Venkatasubramanian, Suresh
author:
- given: Cyrus
  family: Cousins
- given: I.
  family: Elizabeth Kumar
- given: Suresh
  family: Venkatasubramanian
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/cousins24a/cousins24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
