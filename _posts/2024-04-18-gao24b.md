---
title: Decentralized Multi-Level Compositional Optimization Algorithms with Level-Independent
  Convergence Rate
abstract: Stochastic multi-level compositional optimization problems cover many new
  machine learning paradigms, e.g., multi-step model-agnostic meta-learning, which
  require efficient optimization algorithms for large-scale data. This paper studies
  the decentralized stochastic multi-level optimization algorithm, which is challenging
  because the multi-level structure and decentralized communication scheme may make
  the number of levels significantly affect the order of the convergence rate. To
  this end, we develop two novel decentralized optimization algorithms to optimize
  the multi-level compositional optimization problem. Our theoretical results show
  that both algorithms can achieve the level-independent convergence rate for nonconvex
  problems under much milder conditions compared with existing single-machine algorithms.
  To the best of our knowledge, this is the first work that achieves the level-independent
  convergence rate under the decentralized setting. Moreover, extensive experiments
  confirm the efficacy of our proposed algorithms.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: gao24b
month: 0
tex_title: Decentralized Multi-Level Compositional Optimization Algorithms with Level-Independent
  Convergence Rate
firstpage: 4402
lastpage: 4410
page: 4402-4410
order: 4402
cycles: false
bibtex_author: Gao, Hongchang
author:
- given: Hongchang
  family: Gao
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/gao24b/gao24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
