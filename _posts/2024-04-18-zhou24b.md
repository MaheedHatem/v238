---
title: Reward-Relevance-Filtered Linear Offline Reinforcement Learning
software: https://github.com/angelamzhou/reward-filtered-offline-reinforcement-learning
abstract: 'This paper studies offline reinforcement learning with linear function
  approximation in a setting with decision-theoretic, but not estimation sparsity.
  The structural restrictions of the data-generating process presume that the transitions
  factor into a sparse component that affects the reward and could affect additional
  exogenous dynamics that do not affect the reward. Although the minimally sufficient
  adjustment set for estimation of full-state transition properties depends on the
  whole state, the optimal policy and therefore state-action value function depends
  only on the sparse component: we call this causal/decision-theoretic sparsity. We
  develop a method for reward-filtering the estimation of the state-action value function
  to the sparse component by a modification of thresholded lasso in least-squares
  policy evaluation. We provide theoretical guarantees for our reward-filtered linear
  fitted-Q-iteration, with sample complexity depending only on the size of the sparse
  component.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou24b
month: 0
tex_title: Reward-Relevance-Filtered Linear Offline Reinforcement Learning
firstpage: 3025
lastpage: 3033
page: 3025-3033
order: 3025
cycles: false
bibtex_author: Zhou, Angela
author:
- given: Angela
  family: Zhou
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/zhou24b/zhou24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
