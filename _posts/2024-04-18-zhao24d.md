---
title: Self-Supervised Quantization-Aware Knowledge Distillation
software: https://github.com/kaiqi123/SQAKD.git
abstract: 'Quantization-aware training (QAT) and Knowledge Distillation (KD) are combined
  to achieve competitive performance in creating low-bit deep learning models. However,
  existing works applying KD to QAT require tedious hyper-parameter tuning to balance
  the weights of different loss terms, assume the availability of labeled training
  data, and require complex, computationally intensive training procedures for good
  performance. To address these limitations, this paper proposes a novel Self-Supervised
  Quantization-Aware Knowledge Distillation (SQAKD) framework. SQAKD first unifies
  the forward and backward dynamics of various quantization functions, making it flexible
  for incorporating various QAT works. Then it formulates QAT as a co-optimization
  problem that simultaneously minimizes the KL-Loss between the full-precision and
  low-bit models for KD and the discretization error for quantization, without supervision
  from labels. A comprehensive evaluation shows that SQAKD substantially outperforms
  the state-of-the-art QAT and KD works for a variety of model architectures. Our
  code is at: https://github.com/kaiqi123/SQAKD.git.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao24d
month: 0
tex_title: Self-Supervised Quantization-Aware Knowledge Distillation
firstpage: 4375
lastpage: 4383
page: 4375-4383
order: 4375
cycles: false
bibtex_author: Zhao, Kaiqi and Zhao, Ming
author:
- given: Kaiqi
  family: Zhao
- given: Ming
  family: Zhao
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/zhao24d/zhao24d.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
