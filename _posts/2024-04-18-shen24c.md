---
title: Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization
abstract: In recent years, federated minimax optimization has attracted growing interest
  due to its extensive applications in various machine learning tasks. While Smoothed
  Alternative Gradient Descent Ascent (Smoothed-AGDA) has proved successful in centralized
  nonconvex minimax optimization, how and whether smoothing techniques could be helpful
  in a federated setting remains unexplored. In this paper, we propose a new algorithm
  termed Federated Stochastic Smoothed Gradient Descent Ascent (FESS-GDA), which utilizes
  the smoothing technique for federated minimax optimization. We prove that FESS-GDA
  can be uniformly applied to solve several classes of federated minimax problems
  and prove new or better analytical convergence results for these settings. We showcase
  the practical efficiency of FESS-GDA in practical federated learning tasks of training
  generative adversarial networks (GANs) and fair classification.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shen24c
month: 0
tex_title: Stochastic Smoothed Gradient Descent Ascent for Federated Minimax Optimization
firstpage: 3988
lastpage: 3996
page: 3988-3996
order: 3988
cycles: false
bibtex_author: Shen, Wei and Huang, Minhui and Zhang, Jiawei and Shen, Cong
author:
- given: Wei
  family: Shen
- given: Minhui
  family: Huang
- given: Jiawei
  family: Zhang
- given: Cong
  family: Shen
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/shen24c/shen24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
