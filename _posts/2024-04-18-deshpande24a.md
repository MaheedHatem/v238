---
title: Online Calibrated and Conformal Prediction Improves Bayesian Optimization
abstract: Accurate uncertainty estimates are important in sequential model-based decision-making
  tasks such as Bayesian optimization. However, these estimates can be imperfect if
  the data violates assumptions made by the model (e.g., Gaussianity). This paper
  studies which uncertainties are needed in model-based decision-making and in Bayesian
  optimization, and argues that uncertainties can benefit from calibrationâ€”i.e., an
  80% predictive interval should contain the true outcome 80% of the time. Maintaining
  calibration, however, can be challenging when the data is non-stationary and depends
  on our actions. We propose using simple algorithms based on online learning to provably
  maintain calibration on non-i.i.d. data, and we show how to integrate these algorithms
  in Bayesian optimization with minimal overhead. Empirically, we find that calibrated
  Bayesian optimization converges to better optima in fewer steps, and we demonstrate
  improved performance on standard benchmark functions and hyperparameter optimization
  tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: deshpande24a
month: 0
tex_title: Online Calibrated and Conformal Prediction Improves {B}ayesian Optimization
firstpage: 1450
lastpage: 1458
page: 1450-1458
order: 1450
cycles: false
bibtex_author: Deshpande, Shachi and Marx, Charles and Kuleshov, Volodymyr
author:
- given: Shachi
  family: Deshpande
- given: Charles
  family: Marx
- given: Volodymyr
  family: Kuleshov
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/deshpande24a/deshpande24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
