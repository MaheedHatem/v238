---
title: Comparing Comparators in Generalization Bounds
software: https://github.com/fredrikhellstrom/comparing-comparators
abstract: We derive generic information-theoretic and PAC-Bayesian generalization
  bounds involving an arbitrary convex comparator function, which measures the discrepancy
  between the training loss and the population loss. The bounds hold under the assumption
  that the cumulant-generating function (CGF) of the comparator is upper-bounded by
  the corresponding CGF within a family of bounding distributions. We show that the
  tightest possible bound is obtained with the comparator being the convex conjugate
  of the CGF of the bounding distribution, also known as the Cramér function. This
  conclusion applies more broadly to generalization bounds with a similar structure.
  This confirms the near-optimality of known bounds for bounded and sub-Gaussian losses
  and leads to novel bounds under other bounding distributions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hellstrom24a
month: 0
tex_title: Comparing Comparators in Generalization Bounds
firstpage: 73
lastpage: 81
page: 73-81
order: 73
cycles: false
bibtex_author: Hellstr\"{o}m, Fredrik and Guedj, Benjamin
author:
- given: Fredrik
  family: Hellström
- given: Benjamin
  family: Guedj
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/hellstrom24a/hellstrom24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
