---
title: A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning
software: https://github.com/kihyukh/aistats2024
abstract: Offline constrained reinforcement learning (RL) aims to learn a policy that
  maximizes the expected cumulative reward subject to constraints on expected cumulative
  cost using an existing dataset. In this paper, we propose Primal-Dual-Critic Algorithm
  (PDCA), a novel algorithm for offline constrained RL with general function approximation.
  PDCA runs a primal-dual algorithm on the Lagrangian function estimated by critics.
  The primal player employs a no-regret policy optimization oracle to maximize the
  Lagrangian estimate and the dual player acts greedily to minimize the Lagrangian
  estimate. We show that PDCA finds a near saddle point of the Lagrangian, which is
  nearly optimal for the constrained RL problem. Unlike previous work that requires
  concentrability and a strong Bellman completeness assumption, PDCA only requires
  concentrability and realizability assumptions for sample-efficient learning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hong24a
month: 0
tex_title: A Primal-Dual-Critic Algorithm for Offline Constrained Reinforcement Learning
firstpage: 280
lastpage: 288
page: 280-288
order: 280
cycles: false
bibtex_author: Hong, Kihyuk and Li, Yuhang and Tewari, Ambuj
author:
- given: Kihyuk
  family: Hong
- given: Yuhang
  family: Li
- given: Ambuj
  family: Tewari
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/hong24a/hong24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
