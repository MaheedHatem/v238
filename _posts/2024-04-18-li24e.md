---
title: Prior-dependent analysis of posterior sampling reinforcement learning with
  function approximation
abstract: This work advances randomized exploration in reinforcement learning (RL)
  with function approximation modeled by linear mixture MDPs. We establish the first
  prior-dependent Bayesian regret bound for RL with function approximation; and refine
  the Bayesian regret analysis for posterior sampling reinforcement learning (PSRL),
  presenting an upper bound of $\tilde{\mathcal{O}}(d\sqrt{H^3 T \log T})$, where
  $d$ represents the dimensionality of the transition kernel, $H$ the planning horizon,
  and $T$ the total number of interactions. This signifies a methodological enhancement
  by optimizing the $\mathcal{O}(\sqrt{\log T})$ factor over the previous benchmark
  (Osband and Van Roy, 2014) specified to linear mixture MDPs. Our approach, leveraging
  a value-targeted model learning perspective, introduces a decoupling argument and
  a variance reduction technique, moving beyond traditional analyses reliant on confidence
  sets and concentration inequalities to formalize Bayesian regret bounds more effectively.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24e
month: 0
tex_title: Prior-dependent analysis of posterior sampling reinforcement learning with
  function approximation
firstpage: 559
lastpage: 567
page: 559-567
order: 559
cycles: false
bibtex_author: Li, Yingru and Luo, Zhiquan
author:
- given: Yingru
  family: Li
- given: Zhiquan
  family: Luo
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/li24e/li24e.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
