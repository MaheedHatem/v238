---
title: Generalization Bounds for Label Noise Stochastic Gradient Descent
abstract: We develop generalization error bounds for stochastic gradient descent (SGD)
  with label noise in non-convex settings under uniform dissipativity and smoothness
  conditions. Under a suitable choice of semimetric, we establish a contraction in
  Wasserstein distance of the label noise stochastic gradient flow that depends polynomially
  on the parameter dimension $d$. Using the framework of algorithmic stability, we
  derive time-independent generalisation error bounds for the discretized algorithm
  with a constant learning rate. The error bound we achieve scales polynomially with
  $d$ and with the rate of $n^{-2/3}$, where $n$ is the sample size. This rate is
  better than the best-known rate of $n^{-1/2}$ established for stochastic gradient
  Langevin dynamics (SGLD)—which employs parameter-independent Gaussian noise—under
  similar conditions. Our analysis offers quantitative insights into the effect of
  label noise.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: eun-huh24a
month: 0
tex_title: Generalization Bounds for Label Noise Stochastic Gradient Descent
firstpage: 1360
lastpage: 1368
page: 1360-1368
order: 1360
cycles: false
bibtex_author: Eun Huh, Jung and Rebeschini, Patrick
author:
- given: Jung
  family: Eun Huh
- given: Patrick
  family: Rebeschini
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/eun-huh24a/eun-huh24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
