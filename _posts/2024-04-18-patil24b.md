---
title: On learning history-based policies for controlling Markov decision processes
software: https://github.com/gp1702/history-based-policies
abstract: Reinforcement learning (RL) folklore suggests that methods of function approximation
  based on history, such as recurrent neural networks or state abstractions that include
  past information, outperform those without memory, because function approximation
  in Markov decision processes (MDP) can lead to a scenario akin to dealing with a
  partially observable MDP (POMDP). However, formal analysis of history-based algorithms
  has been limited, with most existing frameworks concentrating on features without
  historical context. In this paper, we introduce a theoretical framework to examine
  the behaviour of RL algorithms that control an MDP using feature abstraction mappings
  based on historical data. Additionally, we leverage this framework to develop a
  practical RL algorithm and assess its performance across various continuous control
  tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: patil24b
month: 0
tex_title: On learning history-based policies for controlling {M}arkov decision processes
firstpage: 3511
lastpage: 3519
page: 3511-3519
order: 3511
cycles: false
bibtex_author: Patil, Gandharv and Mahajan, Aditya and Precup, Doina
author:
- given: Gandharv
  family: Patil
- given: Aditya
  family: Mahajan
- given: Doina
  family: Precup
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/patil24b/patil24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
