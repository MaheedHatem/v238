---
title: Why is parameter averaging beneficial in SGD? An objective smoothing perspective
software: https://github.com/anitan0925/averaged_sgd
abstract: It is often observed that stochastic gradient descent (SGD) and its variants
  implicitly select a solution with good generalization performance; such implicit
  bias is often characterized in terms of the sharpness of the minima. Kleinberg et
  al. (2018) connected this bias with the smoothing effect of SGD which eliminates
  sharp local minima by the convolution using the stochastic gradient noise. We follow
  this line of research and study the commonly-used averaged SGD algorithm, which
  has been empirically observed in Izmailov et al. (2018) to prefer a flat minimum
  and therefore achieves better generalization. We prove that in certain problem settings,
  averaged SGD can efficiently optimize the smoothed objective which avoids sharp
  local minima. In experiments, we verify our theory and show that parameter averaging
  with an appropriate step size indeed leads to significant improvement in the performance
  of SGD.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: nitanda24a
month: 0
tex_title: Why is parameter averaging beneficial in {SGD}? An objective smoothing
  perspective
firstpage: 3565
lastpage: 3573
page: 3565-3573
order: 3565
cycles: false
bibtex_author: Nitanda, Atsushi and Kikuchi, Ryuhei and Maeda, Shugo and Wu, Denny
author:
- given: Atsushi
  family: Nitanda
- given: Ryuhei
  family: Kikuchi
- given: Shugo
  family: Maeda
- given: Denny
  family: Wu
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/nitanda24a/nitanda24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
