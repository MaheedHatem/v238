---
title: On the Model-Misspecification in Reinforcement Learning
abstract: 'The success of reinforcement learning (RL) crucially depends on effective
  function approximation when dealing with complex ground-truth models. Existing sample-efficient
  RL algorithms primarily employ three approaches to function approximation: policy-based,
  value-based, and model-based methods. However, in the face of model misspecification—a
  disparity between the ground-truth and optimal function approximators—it is shown
  that policy-based approaches can be robust even when the policy function approximation
  is under a large \emph{locally-bounded} misspecification error, with which the function
  class may exhibit a $\Omega(1)$ approximation error in specific states and actions,
  but remains small on average within a policy-induced state distribution. Yet it
  remains an open question whether similar robustness can be achieved with value-based
  and model-based approaches, especially with general function approximation. To bridge
  this gap, in this paper we present a unified theoretical framework for addressing
  model misspecification in RL. We demonstrate that, through meticulous algorithm
  design and sophisticated analysis, value-based and model-based methods employing
  general function approximation can achieve robustness under local misspecification
  error bounds. In particular, they can attain a regret bound of $\widetilde{O}\left(\mathrm{poly}(dH)\cdot(\sqrt{K}
  + K\cdot\zeta) \right)$, where $d$ represents the complexity of the function class,
  $H$ is the episode length, $K$ is the total number of episodes, and $\zeta$ denotes
  the local bound for misspecification error. Furthermore, we propose an algorithmic
  framework that can achieve the same order of regret bound without prior knowledge
  of $\zeta$, thereby enhancing its practical applicability.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24m
month: 0
tex_title: On the Model-Misspecification in Reinforcement Learning
firstpage: 2764
lastpage: 2772
page: 2764-2772
order: 2764
cycles: false
bibtex_author: Li, Yunfan and Yang, Lin
author:
- given: Yunfan
  family: Li
- given: Lin
  family: Yang
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/li24m/li24m.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
