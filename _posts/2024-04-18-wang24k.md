---
title: On cyclical MCMC sampling
abstract: Cyclical MCMC is a novel MCMC framework recently proposed by Zhang et al.
  (2019) to address the challenge posed by high- dimensional multimodal posterior
  distributions like those arising in deep learning. The algorithm works by generating
  a nonhomogeneous Markov chain that tracks —cyclically in time— tempered versions
  of the target distribution. We show in this work that cyclical MCMC converges to
  the desired probability distribution in settings where the Markov kernels used are
  fast mixing, and sufficiently long cycles are employed. However in the far more
  common settings of slow mixing kernels, the algorithm may fail to produce samples
  from the desired distribution. In particular, in a simple mixture example with unequal
  variance we show by simulation that cyclical MCMC fails to converge to the desired
  limit. Finally, we show that cyclical MCMC typically estimates well the local shape
  of the target distribution around each mode, even when we do not have convergence
  to the target.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang24k
month: 0
tex_title: On cyclical {MCMC} sampling
firstpage: 3817
lastpage: 3825
page: 3817-3825
order: 3817
cycles: false
bibtex_author: Wang, Liwei and Liu, Xinru and Smith, Aaron and Y Atchade, Aguemon
author:
- given: Liwei
  family: Wang
- given: Xinru
  family: Liu
- given: Aaron
  family: Smith
- given: Aguemon
  family: Y Atchade
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/wang24k/wang24k.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
