---
title: Sequence Length Independent Norm-Based Generalization Bounds for Transformers
software: https://github.com/traugerjacob/Transformer-Gen-Bounds
abstract: This paper provides norm-based generalization bounds for the Transformer
  architecture that do not depend on the input sequence length. We employ a covering
  number based approach to prove our bounds. We use three novel covering number bounds
  for the function class of bounded linear mappings to upper bound the Rademacher
  complexity of the Transformer. Furthermore, we show this generalization bound applies
  to the common Transformer training technique of masking and then predicting the
  masked word. We also run a simulated study on a sparse majority data set that empirically
  validates our theoretical findings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: trauger24a
month: 0
tex_title: Sequence Length Independent Norm-Based Generalization Bounds for Transformers
firstpage: 1405
lastpage: 1413
page: 1405-1413
order: 1405
cycles: false
bibtex_author: Trauger, Jacob and Tewari, Ambuj
author:
- given: Jacob
  family: Trauger
- given: Ambuj
  family: Tewari
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/trauger24a/trauger24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
