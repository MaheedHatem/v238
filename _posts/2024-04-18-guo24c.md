---
title: The effect of Leaky ReLUs on the training and generalization of overparameterized
  networks
software: https://github.com/sli743/leakyReLU
abstract: We investigate the training and generalization errors of overparameterized
  neural networks (NNs) with a wide class of leaky rectified linear unit (ReLU) functions.
  More specifically, we carefully upper bound both the convergence rate of the training
  error and the generalization error of such NNs and investigate the dependence of
  these bounds on the Leaky ReLU parameter, $\alpha$. We show that $\alpha =-1$, which
  corresponds to the absolute value activation function, is optimal for the training
  error bound. Furthermore, in special settings, it is also optimal for the generalization
  error bound. Numerical experiments empirically support the practical choices guided
  by the theory.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: guo24c
month: 0
tex_title: The effect of Leaky {ReLUs} on the training and generalization of overparameterized
  networks
firstpage: 4393
lastpage: 4401
page: 4393-4401
order: 4393
cycles: false
bibtex_author: Guo, Yinglong and Li, Shaohan and Lerman, Gilad
author:
- given: Yinglong
  family: Guo
- given: Shaohan
  family: Li
- given: Gilad
  family: Lerman
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/guo24c/guo24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
