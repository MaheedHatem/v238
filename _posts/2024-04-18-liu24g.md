---
title: 'User-level Differentially Private Stochastic Convex Optimization: Efficient
  Algorithms with Optimal Rates'
abstract: We study differentially private stochastic convex optimization (DP-SCO)
  under user-level privacy, where each user may hold multiple data items. Existing
  work for user-level DP-SCO either requires super-polynomial runtime (Ghazi et al.,
  2023) or requires the number of users to grow polynomially with the dimensionality
  of the problem with additional strict assumptions (Bassily et al., 2023). We develop
  new algorithms for user-level DP-SCO that obtain optimal rates for both convex and
  strongly convex functions in polynomial time and require the number of users to
  grow only logarithmically in the dimension. Moreover, our algorithms are the first
  to obtain optimal rates for non-smooth functions in polynomial time. These algorithms
  are based on multiple-pass DP-SGD, combined with a novel private mean estimation
  procedure for concentrated data, which applies an outlier removal step before estimating
  the mean of the gradients.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24g
month: 0
tex_title: 'User-level Differentially Private Stochastic Convex Optimization: Efficient
  Algorithms with Optimal Rates'
firstpage: 4240
lastpage: 4248
page: 4240-4248
order: 4240
cycles: false
bibtex_author: Liu, Daogao and Asi, Hilal
author:
- given: Daogao
  family: Liu
- given: Hilal
  family: Asi
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/liu24g/liu24g.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
