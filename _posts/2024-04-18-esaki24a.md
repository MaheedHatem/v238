---
title: Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex
abstract: Classification models based on deep neural networks (DNNs) must be calibrated
  to measure the reliability of predictions. Some recent calibration methods have
  employed a probabilistic model on the probability simplex. However, these calibration
  methods cannot preserve the accuracy of pre-trained models, even those with a high
  classification accuracy. We propose an accuracy-preserving calibration method using
  the Concrete distribution as the probabilistic model on the probability simplex.
  We theoretically prove that a DNN model trained on cross-entropy loss has optimality
  as the parameter of the Concrete distribution. We also propose an efficient method
  that synthetically generates samples for training probabilistic models on the probability
  simplex. We demonstrate that the proposed method can outperform previous methods
  in accuracy-preserving calibration tasks using benchmarks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: esaki24a
month: 0
tex_title: Accuracy-Preserving Calibration via Statistical Modeling on Probability
  Simplex
firstpage: 1666
lastpage: 1674
page: 1666-1674
order: 1666
cycles: false
bibtex_author: Esaki, Yasushi and Nakamura, Akihiro and Kawano, Keisuke and Tokuhisa,
  Ryoko and Kutsuna, Takuro
author:
- given: Yasushi
  family: Esaki
- given: Akihiro
  family: Nakamura
- given: Keisuke
  family: Kawano
- given: Ryoko
  family: Tokuhisa
- given: Takuro
  family: Kutsuna
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/esaki24a/esaki24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
