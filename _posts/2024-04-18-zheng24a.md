---
title: Better Batch for Deep Probabilistic Time Series Forecasting
abstract: Deep probabilistic time series forecasting has gained attention for its
  ability to provide nonlinear approximation and valuable uncertainty quantification
  for decision-making. However, existing models often oversimplify the problem by
  assuming a time-independent error process and overlooking serial correlation. To
  overcome this limitation, we propose an innovative training method that incorporates
  error autocorrelation to enhance probabilistic forecasting accuracy. Our method
  constructs a mini-batch as a collection of D consecutive time series segments for
  model training. It explicitly learns a time-varying covariance matrix over each
  mini-batch, encoding error correlation among adjacent time steps. The learned covariance
  matrix can be used to improve prediction accuracy and enhance uncertainty quantification.
  We evaluate our method on two different neural forecasting models and multiple public
  datasets. Experimental results confirm the effectiveness of the proposed approach
  in improving the performance of both models across a range of datasets, resulting
  in notable improvements in predictive accuracy.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zheng24a
month: 0
tex_title: Better Batch for Deep Probabilistic Time Series Forecasting
firstpage: 91
lastpage: 99
page: 91-99
order: 91
cycles: false
bibtex_author: Zheng, Zhihao and Choi, Seongjin and Sun, Lijun
author:
- given: Zhihao
  family: Zheng
- given: Seongjin
  family: Choi
- given: Lijun
  family: Sun
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/zheng24a/zheng24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
