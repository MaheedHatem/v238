---
title: 'Better Representations via Adversarial Training in Pre-Training: A Theoretical
  Perspective'
abstract: Pre-training is known to generate universal representations for downstream
  tasks in large-scale deep learning such as large language models. Existing literature,
  e.g., Kim et al. (2020), empirically observe that the downstream tasks can inherit
  the adversarial robustness of the pre-trained model. We provide theoretical justifications
  for this robustness inheritance phenomenon. Our theoretical results reveal that
  feature purification plays an important role in connecting the adversarial robustness
  of the pre-trained model and the downstream tasks in two-layer neural networks.
  Specifically, we show that (i) with adversarial training, each hidden node tends
  to pick only one (or a few) feature; (ii) without adversarial training, the hidden
  nodes can be vulnerable to attacks. This observation is valid for both supervised
  pre-training and contrastive learning. With purified nodes, it turns out that clean
  training is enough to achieve adversarial robustness in downstream tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xing24a
month: 0
tex_title: 'Better Representations via Adversarial Training in Pre-Training: A Theoretical
  Perspective'
firstpage: 199
lastpage: 207
page: 199-207
order: 199
cycles: false
bibtex_author: Xing, Yue and Lin, Xiaofeng and Song, Qifan and Xu, Yi and Zeng, Belinda
  and Cheng, Guang
author:
- given: Yue
  family: Xing
- given: Xiaofeng
  family: Lin
- given: Qifan
  family: Song
- given: Yi
  family: Xu
- given: Belinda
  family: Zeng
- given: Guang
  family: Cheng
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/xing24a/xing24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
