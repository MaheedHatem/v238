---
title: Mechanics of Next Token Prediction with Self-Attention
abstract: 'Transformer-based language models are trained on large datasets to predict
  the next token given an input sequence. Despite this simple training objective,
  they have led to revolutionary advances in natural language processing. Underlying
  this success is the self-attention mechanism. In this work, we ask: What does a
  single self-attention layer learn from next-token prediction? We show that training
  self-attention with gradient descent learns an automaton which generates the next
  token in two distinct steps: (1) Hard retrieval: Given input sequence, self-attention
  precisely selects the high-priority input tokens associated with the last input
  token. (2) Soft composition: It then creates a convex combination of the high-priority
  tokens from which the next token can be sampled. Under suitable conditions, we rigorously
  characterize these mechanics through a directed graph over tokens extracted from
  the training data. We prove that gradient descent implicitly discovers the strongly-connected
  components (SCC) of this graph and self-attention learns to retrieve the tokens
  that belong to the highest-priority SCC available in the context window. Our theory
  relies on decomposing the model weights into a directional component and a finite
  component that correspond to hard retrieval and soft composition steps respectively.
  This also formalizes a related implicit bias formula conjectured in [Tarzanagh et
  al. 2023]. We hope that these findings shed light on how self-attention processes
  sequential data and pave the path toward demystifying more complex architectures.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li24f
month: 0
tex_title: Mechanics of Next Token Prediction with Self-Attention
firstpage: 685
lastpage: 693
page: 685-693
order: 685
cycles: false
bibtex_author: Li, Yingcong and Huang, Yixiao and Ildiz, Muhammed E. and Singh Rawat,
  Ankit and Oymak, Samet
author:
- given: Yingcong
  family: Li
- given: Yixiao
  family: Huang
- given: Muhammed E.
  family: Ildiz
- given: Ankit
  family: Singh Rawat
- given: Samet
  family: Oymak
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/li24f/li24f.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
