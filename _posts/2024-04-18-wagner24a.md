---
title: 'Diagonalisation SGD: Fast & Convergent SGD for Non-Differentiable Models via
  Reparameterisation and Smoothing'
software: https://github.com/domwagner/DSGD.git
abstract: 'It is well-known that the reparameterisation gradient estimator, which
  exhibits low variance in practice, is biased for non-differentiable models. This
  may compromise correctness of gradient-based optimisation methods such as stochastic
  gradient descent (SGD). We introduce a simple syntactic framework to define non-differentiable
  functions piecewisely and present a systematic approach to obtain smoothings for
  which the reparameterisation gradient estimator is unbiased. Our main contribution
  is a novel variant of SGD, Diagonalisation Stochastic Gradient Descent, which progressively
  enhances the accuracy of the smoothed approximation during optimisation, and we
  prove convergence to stationary points of the unsmoothed (original) objective. Our
  empirical evaluation reveals benefits over the state of the art: our approach is
  simple, fast, stable and attains orders of magnitude reduction in work-normalised
  variance.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wagner24a
month: 0
tex_title: 'Diagonalisation {SGD}: Fast & Convergent {SGD} for Non-Differentiable
  Models via Reparameterisation and Smoothing'
firstpage: 1801
lastpage: 1809
page: 1801-1809
order: 1801
cycles: false
bibtex_author: Wagner, Dominik and Khajwal, Basim and Ong, Luke
author:
- given: Dominik
  family: Wagner
- given: Basim
  family: Khajwal
- given: Luke
  family: Ong
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/wagner24a/wagner24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
