---
title: Acceleration and Implicit Regularization in Gaussian Phase Retrieval
software: https://github.com/twmaunu/accelerated-phase-retrieval
abstract: We study accelerated optimization methods in the Gaussian phase retrieval
  problem. In this setting, we prove that gradient methods with Polyak or Nesterov
  momentum have similar implicit regularization to gradient descent. This implicit
  regularization ensures that the algorithms remain in a nice region, where the cost
  function is strongly convex and smooth despite being nonconvex in general. This
  ensures that these accelerated methods achieve faster rates of convergence than
  gradient descent. Experimental evidence demonstrates that the accelerated methods
  converge faster than gradient descent in practice.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: maunu24a
month: 0
tex_title: Acceleration and Implicit Regularization in {G}aussian Phase Retrieval
firstpage: 4060
lastpage: 4068
page: 4060-4068
order: 4060
cycles: false
bibtex_author: Maunu, Tyler and Molina-Fructuoso, Martin
author:
- given: Tyler
  family: Maunu
- given: Martin
  family: Molina-Fructuoso
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/maunu24a/maunu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
