---
title: Deep Classifier Mimicry without Data Access
software: https://github.com/ml-research/CAKE
abstract: Access to pre-trained models has recently emerged as a standard across numerous
  machine learning domains. Unfortunately, access to the original data the models
  were trained on may not equally be granted. This makes it tremendously challenging
  to fine-tune, compress models, adapt continually, or to do any other type of data-driven
  update. We posit that original data access may however not be required. Specifically,
  we propose Contrastive Abductive Knowledge Extraction (CAKE), a model-agnostic knowledge
  distillation procedure that mimics deep classifiers without access to the original
  data. To this end, CAKE generates pairs of noisy synthetic samples and diffuses
  them contrastively toward a model’s decision boundary. We empirically corroborate
  CAKE’s effectiveness using several benchmark datasets and various architectural
  choices, paving the way for broad application.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: braun24b
month: 0
tex_title: Deep Classifier Mimicry without Data Access
firstpage: 4762
lastpage: 4770
page: 4762-4770
order: 4762
cycles: false
bibtex_author: Braun, Steven and Mundt, Martin and Kersting, Kristian
author:
- given: Steven
  family: Braun
- given: Martin
  family: Mundt
- given: Kristian
  family: Kersting
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/braun24b/braun24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
