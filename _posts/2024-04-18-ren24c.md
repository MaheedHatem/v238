---
title: Understanding the Generalization Benefits of Late Learning Rate Decay
software: https://github.com/yinuoren/landscape
abstract: Why do neural networks trained with large learning rates for longer time
  often lead to better generalization? In this paper, we delve into this question
  by examining the relation between training and testing loss in neural networks.
  Through visualization of these losses, we note that the training trajectory with
  a large learning rate navigates through the minima manifold of the training loss,
  finally nearing the neighborhood of the testing loss minimum. Motivated by these
  findings, we introduce a nonlinear model whose loss landscapes mirror those observed
  for real neural networks. Upon investigating the training process using SGD on our
  model, we demonstrate that an extended phase with a large learning rate steers our
  model towards the minimum norm solution of the training loss, which may achieve
  near-optimal generalization, thereby affirming the empirically observed benefits
  of late learning rate decay.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ren24c
month: 0
tex_title: Understanding the Generalization Benefits of Late Learning Rate Decay
firstpage: 4465
lastpage: 4473
page: 4465-4473
order: 4465
cycles: false
bibtex_author: Ren, Yinuo and Ma, Chao and Ying, Lexing
author:
- given: Yinuo
  family: Ren
- given: Chao
  family: Ma
- given: Lexing
  family: Ying
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/ren24c/ren24c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
