---
title: 'Free-form Flows: Make Any Architecture a Normalizing Flow'
software: https://github.com/vislearn/FFF
abstract: Normalizing Flows are generative models that directly maximize the likelihood.
  Previously, the design of normalizing flows was largely constrained by the need
  for analytical invertibility. We overcome this constraint by a training procedure
  that uses an efficient estimator for the gradient of the change of variables formula.
  This enables any dimension-preserving neural network to serve as a generative model
  through maximum likelihood training. Our approach allows placing the emphasis on
  tailoring inductive biases precisely to the task at hand. Specifically, we achieve
  excellent results in molecule generation benchmarks utilizing E(n)-equivariant networks
  at greatly improved sampling speed. Moreover, our method is competitive in an inverse
  problem benchmark, while employing off-the-shelf ResNet architectures. We publish
  our code at https://github.com/vislearn/FFF.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: draxler24a
month: 0
tex_title: 'Free-form Flows: Make Any Architecture a Normalizing Flow'
firstpage: 2197
lastpage: 2205
page: 2197-2205
order: 2197
cycles: false
bibtex_author: Draxler, Felix and Sorrenson, Peter and Zimmermann, Lea and Rousselot,
  Armand and K\"{o}the, Ullrich
author:
- given: Felix
  family: Draxler
- given: Peter
  family: Sorrenson
- given: Lea
  family: Zimmermann
- given: Armand
  family: Rousselot
- given: Ullrich
  family: KÃ¶the
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/draxler24a/draxler24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
