---
title: Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization Problems
software: https://github.com/Kutuz4/AISTATS2024_SMoM
abstract: We consider stochastic optimization problems with heavy-tailed noise with
  structured density. For such problems, we show that it is possible to get faster
  rates of convergence than $O(K^{-2(\alpha - 1) / \alpha})$, when the stochastic
  gradients have finite $\alpha$-th moment, $\alpha \in (1, 2]$. In particular, our
  analysis allows the noise norm to have an unbounded expectation. To achieve these
  results, we stabilize stochastic gradients, using smoothed medians of means. We
  prove that the resulting estimates have negligible bias and controllable variance.
  This allows us to carefully incorporate them into clipped-SGD and clipped-SSTM and
  derive new high-probability complexity bounds in the considered setup.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: puchkin24a
month: 0
tex_title: Breaking the Heavy-Tailed Noise Barrier in Stochastic Optimization Problems
firstpage: 856
lastpage: 864
page: 856-864
order: 856
cycles: false
bibtex_author: Puchkin, Nikita and Gorbunov, Eduard and Kutuzov, Nickolay and Gasnikov,
  Alexander
author:
- given: Nikita
  family: Puchkin
- given: Eduard
  family: Gorbunov
- given: Nickolay
  family: Kutuzov
- given: Alexander
  family: Gasnikov
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/puchkin24a/puchkin24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
