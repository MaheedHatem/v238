---
title: On the Theoretical Expressive Power and the Design Space of Higher-Order Graph
  Transformers
software: https://github.com/zhouc20/k-Transformer
abstract: Graph transformers have recently received significant attention in graph
  learning, partly due to their ability to capture more global interaction via self-attention.
  Nevertheless, while higher-order graph neural networks have been reasonably well
  studied, the exploration of extending graph transformers to higher-order variants
  is just starting. Both theoretical understanding and empirical results are limited.
  In this paper, we provide a systematic study of the theoretical expressive power
  of order-$k$ graph transformers and sparse variants. We first show that, an order-$k$
  graph transformer without additional structural information is less expressive than
  the $k$-Weisfeiler Lehman ($k$-WL) test despite its high computational cost. We
  then explore strategies to both sparsify and enhance the higher-order graph transformers,
  aiming to improve both their efficiency and expressiveness. Indeed, sparsification
  based on neighborhood information can enhance the expressive power, as it provides
  additional information about input graph structures. In particular, we show that
  a natural neighborhood-based sparse order-$k$ transformer model is not only computationally
  efficient, but also expressive â€“ as expressive as $k$-WL test. We further study
  several other sparse graph attention models that are computationally efficient and
  provide their expressiveness analysis. Finally, we provide experimental results
  to show the effectiveness of the different sparsification strategies.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhou24a
month: 0
tex_title: On the Theoretical Expressive Power and the Design Space of Higher-Order
  Graph Transformers
firstpage: 2179
lastpage: 2187
page: 2179-2187
order: 2179
cycles: false
bibtex_author: Zhou, Cai and Yu, Rose and Wang, Yusu
author:
- given: Cai
  family: Zhou
- given: Rose
  family: Yu
- given: Yusu
  family: Wang
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/zhou24a/zhou24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
