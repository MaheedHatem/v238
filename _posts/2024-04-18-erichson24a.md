---
title: 'NoisyMix: Boosting Model Robustness to Common Corruptions'
software: https://github.com/erichson/NoisyMix
abstract: The robustness of neural networks has become increasingly important in real-world
  applications where stable and reliable performance is valued over simply achieving
  high predictive accuracy. To address this, data augmentation techniques have been
  shown to improve robustness against input perturbations and domain shifts. In this
  paper, we propose a new training scheme called NoisyMix that leverages noisy augmentations
  in both input and feature space to improve model robustness and in-domain accuracy.
  We demonstrate the effectiveness of NoisyMix on several benchmark datasets, including
  ImageNet-C, ImageNet-R, and ImageNet-P. Additionally, we provide theoretical analysis
  to better understand the implicit regularization and robustness properties of NoisyMix.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: erichson24a
month: 0
tex_title: "{NoisyMix}: Boosting Model Robustness to Common Corruptions"
firstpage: 4033
lastpage: 4041
page: 4033-4041
order: 4033
cycles: false
bibtex_author: Erichson, Benjamin and Hoe Lim, Soon and Xu, Winnie and Utrera, Francisco
  and Cao, Ziang and Mahoney, Michael
author:
- given: Benjamin
  family: Erichson
- given: Soon
  family: Hoe Lim
- given: Winnie
  family: Xu
- given: Francisco
  family: Utrera
- given: Ziang
  family: Cao
- given: Michael
  family: Mahoney
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/erichson24a/erichson24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
