---
title: Robust Offline Reinforcement Learning with Heavy-Tailed Rewards
software: https://github.com/Mamba413/ROOM
abstract: This paper endeavors to augment the robustness of offline reinforcement
  learning (RL) in scenarios laden with heavy-tailed rewards, a prevalent circumstance
  in real-world applications. We propose two algorithmic frameworks, ROAM and ROOM,
  for robust off-policy evaluation and offline policy optimization (OPO), respectively.
  Central to our frameworks is the strategic incorporation of the median-of-means
  method with offline RL, enabling straightforward uncertainty estimation for the
  value function estimator. This not only adheres to the principle of pessimism in
  OPO but also adeptly manages heavy-tailed rewards. Theoretical results and extensive
  experiments demonstrate that our two frameworks outperform existing methods on the
  logged dataset exhibits heavy-tailed reward distributions. The implementation of
  the proposal is available at \url{https://github.com/Mamba413/ROOM}.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhu24a
month: 0
tex_title: Robust Offline Reinforcement Learning with Heavy-Tailed Rewards
firstpage: 541
lastpage: 549
page: 541-549
order: 541
cycles: false
bibtex_author: Zhu, Jin and Wan, Runzhe and Qi, Zhengling and Luo, Shikai and Shi,
  Chengchun
author:
- given: Jin
  family: Zhu
- given: Runzhe
  family: Wan
- given: Zhengling
  family: Qi
- given: Shikai
  family: Luo
- given: Chengchun
  family: Shi
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/zhu24a/zhu24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
