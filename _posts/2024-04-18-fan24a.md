---
title: Fast and Adversarial Robust Kernelized SDU Learning
abstract: SDU learning, a weakly supervised learning problem with only pairwise similarities,
  dissimilarities data points and unlabeled data available, has many practical applications.
  However, it is still lacking in defense against adversarial samples, and its learning
  process can be expensive. To address this gap, we propose a novel adversarial training
  framework for SDU learning. Our approach reformulates the conventional minimax problem
  as an equivalent minimization problem based on the kernel perspective, departing
  from traditional confrontational training methods. Additionally, we employ the random
  gradient method and random features to accelerate the training process. Theoretical
  analysis shows that our method can converge to a stationary point at a rate of $\mathcal{O}(1/T^{1/4})$.
  Our experimental results show that our algorithm is superior to other adversarial
  training methods in terms of generalization, efficiency and scalability against
  various adversarial attacks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: fan24a
month: 0
tex_title: Fast and Adversarial Robust Kernelized SDU Learning
firstpage: 1153
lastpage: 1161
page: 1153-1161
order: 1153
cycles: false
bibtex_author: Fan, Yajing and shi, wanli and Chang, Yi and Gu, Bin
author:
- given: Yajing
  family: Fan
- given: wanli
  family: shi
- given: Yi
  family: Chang
- given: Bin
  family: Gu
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/fan24a/fan24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
