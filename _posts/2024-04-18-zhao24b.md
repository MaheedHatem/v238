---
title: On Feynman-Kac training of partial Bayesian neural networks
software: https://github.com/spdes/pbnn
abstract: Recently, partial Bayesian neural networks (pBNNs), which only consider
  a subset of the parameters to be stochastic, were shown to perform competitively
  with full Bayesian neural networks. However, pBNNs are often multi-modal in the
  latent variable space and thus challenging to approximate with parametric models.
  To address this problem, we propose an efficient sampling-based training strategy,
  wherein the training of a pBNN is formulated as simulating a Feynman-Kac model.
  We then describe variations of sequential Monte Carlo samplers that allow us to
  simultaneously estimate the parameters and the latent posterior distribution of
  this model at a tractable computational cost. Using various synthetic and real-world
  datasets we show that our proposed training scheme outperforms the state of the
  art in terms of predictive performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao24b
month: 0
tex_title: On {F}eynman-{K}ac training of partial {B}ayesian neural networks
firstpage: 3223
lastpage: 3231
page: 3223-3231
order: 3223
cycles: false
bibtex_author: Zhao, Zheng and Mair, Sebastian and B. Sch\"{o}n, Thomas and Sj\"{o}lund,
  Jens
author:
- given: Zheng
  family: Zhao
- given: Sebastian
  family: Mair
- given: Thomas
  family: B. Schön
- given: Jens
  family: Sjölund
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/zhao24b/zhao24b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
