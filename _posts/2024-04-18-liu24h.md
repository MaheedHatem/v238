---
title: Mitigating Underfitting in Learning to Defer with Consistent Losses
abstract: Learning to defer (L2D) allows the classifier to defer its prediction to
  an expert for safer predictions, by balancing the systemâ€™s accuracy and extra costs
  incurred by consulting the expert. Various loss functions have been proposed for
  L2D, but they were shown to cause the underfitting of trained classifiers when extra
  consulting costs exist, resulting in degraded performance. In this paper, we propose
  a novel loss formulation that can mitigate the underfitting issue while remaining
  the statistical consistency. We first show that our formulation can avoid a common
  characteristic shared by most existing losses, which has been shown to be a cause
  of underfitting, and show that it can be combined with the representative losses
  for L2D to enhance their performance and yield consistent losses. We further study
  the regret transfer bounds of the proposed losses and experimentally validate its
  improvements over existing methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu24h
month: 0
tex_title: Mitigating Underfitting in Learning to Defer with Consistent Losses
firstpage: 4816
lastpage: 4824
page: 4816-4824
order: 4816
cycles: false
bibtex_author: Liu, Shuqi and Cao, Yuzhou and Zhang, Qiaozhen and Feng, Lei and An,
  Bo
author:
- given: Shuqi
  family: Liu
- given: Yuzhou
  family: Cao
- given: Qiaozhen
  family: Zhang
- given: Lei
  family: Feng
- given: Bo
  family: An
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/liu24h/liu24h.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
