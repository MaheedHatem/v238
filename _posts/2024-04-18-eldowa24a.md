---
title: General Tail Bounds for Non-Smooth Stochastic Mirror Descent
software: https://github.com/apaudice/AISTATS2024
abstract: 'In this paper, we provide novel tail bounds on the optimization error of
  Stochastic Mirror Descent for convex and Lipschitz objectives. Our analysis extends
  the existing tail bounds from the classical light-tailed Sub-Gaussian noise case
  to heavier-tailed noise regimes. We study the optimization error of the last iterate
  as well as the average of the iterates. We instantiate our results in two important
  cases: a class of noise with exponential tails and one with polynomial tails. A
  remarkable feature of our results is that they do not require an upper bound on
  the diameter of the domain. Finally, we support our theory with illustrative experiments
  that compare the behavior of the average of the iterates with that of the last iterate
  in heavy-tailed noise regimes.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: eldowa24a
month: 0
tex_title: General Tail Bounds for Non-Smooth Stochastic Mirror Descent
firstpage: 3205
lastpage: 3213
page: 3205-3213
order: 3205
cycles: false
bibtex_author: Eldowa, Khaled and Paudice, Andrea
author:
- given: Khaled
  family: Eldowa
- given: Andrea
  family: Paudice
date: 2024-04-18
address:
container-title: Proceedings of The 27th International Conference on Artificial Intelligence
  and Statistics
volume: '238'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 4
  - 18
pdf: https://proceedings.mlr.press/v238/eldowa24a/eldowa24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
